# html-uri-crawler

A randomized web crawler that collects unique HTML URIs that are larger than 1000 bytes, starting from a given seed page.

# Project Overview

This program is an improved version of the (finish) 

The original requirements (finish with bullet points)

What has been been added (finsh with bullet points)
# Getting Started

### Dependencies(finish entire section)

* Describe any prerequisites, libraries, OS version, etc., needed before installing program.
* ex. Windows 10

### Installing

* How/where to download your program
* Any modifications needed to be made to files/folders

### Executing program

* How to run the program
* Step-by-step bullets
```
code blocks for commands
```


## Authors

**Author:** Emily Nowak

*Based on **"Homework 1 - Web Science Intro."** by **Prof. Nasreen Arif** for CS:432 Web Science, at Old Dominion University* 

## Version History

* 0.1
    * Initial Release


## Acknowledgments

### Original Assignment

[HW1 - Web Science Intro. - Question 3 Origninal Instructions](https://github.com/emxily/html-uri-crawler/blob/cec0cd57c59a0b7e5bf42890bf14d824860829a8/original-assignment-intructions.md)

* ***Note:**  Only Question 3 applies to this program implementation.*

### References
* Markdown Syntax: <https://www.markdownguide.org/basic-syntax/#headings>
* HTML Syntax: <https://www.w3schools.com/tags>
* Beautiful Soup Documentation: <https://www.crummy.com/software/BeautifulSoup/bs4/doc>
* Regex for http/https: <https://stackoverflow.com/questions/4643142/regex-to-test-if-string-begins-with-http-or-https>